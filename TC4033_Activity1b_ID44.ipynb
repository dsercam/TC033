{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dsercam/TC033/blob/main/TC4033_Activity1b_ID44.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<font color='darkorange'><b> TC 5033 :: Advanced Machine Learning Methods </b> </font>\n",
        "### <font color='darkgray'><b> Activity 1B :: Implementing a Fully Connected Network for *Kaggle ASL Dataset* </b></font></br></br>\n",
        "###<font color='darkblue'><b>  Group 44 </b></font>\n",
        "***Dante Rodrigo Serna Camarillo A01182676***</br>\n",
        "***Axel Alejandro Tlatoa Villavicencio A01363351***</br>\n",
        "***Carlos Roberto Torres Ferguson A01215432***</br>\n",
        "***Felipe de Jesús Gastélum Lizárraga A01114918***"
      ],
      "metadata": {
        "id": "xFnLeTs8NHhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from io import StringIO\n",
        "import numpy as np\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 as cv\n",
        "import os\n",
        "import math"
      ],
      "metadata": {
        "id": "lK6nrMPKjpyK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **1. Import data set** </font>\n",
        ">>Data sets were loaded into Google drive and exposed to the public.\n",
        "</br>\n",
        ">>Then we use the *requests* method to retrieve the files using the downloads URLs."
      ],
      "metadata": {
        "id": "EYDA7CKEOS46"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rkClsa22eaFb"
      },
      "outputs": [],
      "source": [
        "url = 'https://drive.google.com/uc?export=download&id=1ttUEqCDuNFIenzrr4kMCHkbvBIy9fHmy'\n",
        "ASLTrainSet = requests.get(url)\n",
        "\n",
        "url = 'https://drive.google.com/uc?export=download&id=1mbEOJQKvhw41DpBzBB9PSfPr3IsngfA7'\n",
        "ASLValidationSet = requests.get(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> Get the streamed data into a String format, then read it as CSV format using *read_csv*, assign them to two data frames.\n",
        "</Br>\n",
        ">>> 1. <font color=\"darkorange\"><b> *trainingDf* </b></font> contains the data of the training file\n",
        ">>> 2. <font color=\"darkorange\"><b> *validationDf* </b></font> contains the data of the validation file\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zMSs75XtPebc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainingAsText = StringIO(ASLTrainSet.text)\n",
        "validationAsText = StringIO(ASLValidationSet.text)\n",
        "\n",
        "trainingDf = pd.read_csv(trainingAsText)\n",
        "validationDf = pd.read_csv(validationAsText)"
      ],
      "metadata": {
        "id": "OPXk434LkNKC"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> We can see the size of both sets."
      ],
      "metadata": {
        "id": "Hz0Yc7S5FQFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training set shape :: ',trainingDf.shape)\n",
        "print('Validation set shape :: ',validationDf.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26xOokaI0j1Y",
        "outputId": "2c9c7d2c-0a64-48ed-9101-3794f15138c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set shape ::  (27455, 785)\n",
            "Validation set shape ::  (7172, 785)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **2. Clean Up data frames.** </font>\n",
        ">>> Extract classification (*labels* column in both dataframes)\n",
        ">>> After getting the classification, remove the link *label* column on both dataframes. This will only leave pixels information\n",
        "</br>\n",
        ">>> **y_train** :: (y) traning vector\n",
        "</br>\n",
        ">>> **y_validation** :: (y) validation vector"
      ],
      "metadata": {
        "id": "raj7AfO1uAvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Obtain classification for (y) for both sets on separate arrays\n",
        "y_train = np.array(trainingDf['label'])\n",
        "y_validation = np.array(validationDf['label'])\n",
        "\n",
        "# Remove [label] column in both data frames\n",
        "del trainingDf['label']\n",
        "del validationDf['label']\n",
        "\n",
        "#set data type as float in order to process it\n",
        "x_train = trainingDf.values.astype(np.float32)\n",
        "x_validation = validationDf.values.astype(np.float32)"
      ],
      "metadata": {
        "id": "mjYLSlitrhI9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **3. Prepare plotting function.** </font>\n",
        ">> Plot will help us graph an entry to see the ASL symbol."
      ],
      "metadata": {
        "id": "9CBUzrYHGX6s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_ASL(image):\n",
        "    #pixels = trainingDf.iloc[n,].to_numpy().astype('uint8') #this could have been used to parametrize the method in a different way.\n",
        "    pixels = image.astype('uint8') # set type to uint8 for it to be graphed properly\n",
        "    pixels = pixels.reshape(28,28) # reshape the row to (28x28)\n",
        "    plt.figure(figsize=(5,5))\n",
        "    plt.imshow(pixels.squeeze(), cmap=plt.get_cmap('gray'))\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "hDj3f_9NlctU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ">> Testing our plotting function using a random index."
      ],
      "metadata": {
        "id": "_IBOqzNDHJB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_idx = np.random.randint(len(y_validation))\n",
        "print(f'ASL Symbol: {y_validation[rnd_idx, ]}')\n",
        "plot_ASL(x_validation[rnd_idx,])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "9nplynY-qx4k",
        "outputId": "504a44da-59dc-4bd0-f65f-2ce0b7d90260"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASL Symbol: 11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **4. About the datasets.** </font>\n",
        ">> Some information about our datasets:\n",
        ">>> **Current distribution:**\n"
      ],
      "metadata": {
        "id": "hAZ7e5a1IoPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "totalSamples = trainingDf.shape[0] + validationDf.shape[0] # total samples, including both sets\n",
        "print('Current distirbution of our datasets:')\n",
        "print(' Size of the training set: ',round((trainingDf.shape[0] *100)/totalSamples,2),'%')\n",
        "print(' Size of the validation set: ',round((validationDf.shape[0]*100)/totalSamples,2),'%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiBaSEl2khcO",
        "outputId": "0bdb6379-62ff-4fe3-efdf-3c9f2d60a435"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current distirbution of our datasets:\n",
            " Size of the training set:  79.29 %\n",
            " Size of the validation set:  20.71 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **4.1 Create function to split a dataset given a percentage** </font>\n",
        "> We will use this function to split our order *validation* set into two new sets, a new *test* set and a *validation* set.\n",
        ">> - Method splits dataset in half (50%) by default. Parameter is defined as *pct* at method signature.\n",
        "</br>\n",
        ">> - Method shuffles given data set by default. Parameter is defined as *shuffle* at method signature."
      ],
      "metadata": {
        "id": "gS0muhNBK6tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_val_test(x, y, pct=0.5, shuffle=True):\n",
        "    #Validate the size of provided arguments, stop processing if number of records does not match.\n",
        "    #Throws error if samples and y vector does not match.\n",
        "    assert x.shape[0] == y.shape[0], 'Dataframe does not contain the same numer of records as Y (validation array)'\n",
        "\n",
        "    # Number of samples, log to output\n",
        "    dataCount = x.shape[0]\n",
        "    print('Samples size as ', dataCount)\n",
        "\n",
        "    #shuffle depending on shuffle parameter, shuffle True by default on method signature\n",
        "    if shuffle:\n",
        "        #generate a ndarray that conatins an entry/index for each row of the data\n",
        "        idxs = np.arange(dataCount)\n",
        "        #Shuffle the indexes\n",
        "        np.random.shuffle(idxs)\n",
        "        x = x[idxs]\n",
        "        y = y[idxs]\n",
        "\n",
        "    #Describe how are going to split the given data set, based on the provided pct parameter.\n",
        "    #If the PCT does not result in a intger count, we round up to the next integer.\n",
        "    print('Will split data set at ', pct ,'%.')\n",
        "    if((dataCount*pct).is_integer()) : print(pct, '% of the total records = ',dataCount*pct)\n",
        "    else: print(pct, '% of the total records = ',dataCount*pct, ' will round up to next integer (',math.ceil(dataCount*pct),').')\n",
        "\n",
        "    #creat a copy of the data set (both the samples and y vector), these structures will be used as the 'test' sets.\n",
        "    x_test = x.copy()\n",
        "    y_test = y.copy()\n",
        "\n",
        "    x_val = x[:math.ceil(dataCount*pct)-1].reshape(math.ceil(dataCount*pct)-1, -1).astype(float)\n",
        "    y_val = y[:math.ceil(dataCount*pct)-1].reshape(math.ceil(dataCount*pct)-1, 1)\n",
        "\n",
        "    x_test = x_test[math.trunc(dataCount*pct):].reshape(math.ceil(dataCount*pct), -1).astype(float)\n",
        "    y_test = y_test[math.trunc(dataCount*pct):].reshape(math.ceil(dataCount*pct), 1)\n",
        "    pass\n",
        "    #return validation sets, followed by test sets\n",
        "    return x_val, y_val, x_test, y_test\n"
      ],
      "metadata": {
        "id": "8ONFOoT3rn3h"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **4.2 Use the _split_val_test()_ function to split our validation set into a validation and testing data sets** </font>\n"
      ],
      "metadata": {
        "id": "Ly1xQDmZOQnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val, y_val, x_test, y_test = split_val_test(x_validation, y_validation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udv3PK8iwvfB",
        "outputId": "1645df5e-80e4-4e5d-b437-29db9d98f970"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples size as  7172\n",
            "Will split data set at  0.5 %.\n",
            "0.5 % of the total records =  3586.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> - Our final distibution of data sets as:"
      ],
      "metadata": {
        "id": "BoH3U8AFOwfV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('[Y] Training set shape: ',y_train.shape)\n",
        "print('[x] Training set shape: ',x_train.shape)\n",
        "print('[Y] Validation set shape: ',y_val.shape)\n",
        "print('[x] validation set shape: ',x_val.shape)\n",
        "print('[Y] Test set shape: ',y_test.shape)\n",
        "print('[x] Test set shape: ',x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP0DqERfEGok",
        "outputId": "55751757-79ff-4eeb-b96c-a20ac630ce17"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Y] Training set shape:  (27455,)\n",
            "[x] Training set shape:  (27455, 784)\n",
            "[Y] Validation set shape:  (3585, 1)\n",
            "[x] validation set shape:  (3585, 784)\n",
            "[Y] Test set shape:  (3586, 1)\n",
            "[x] Test set shape:  (3586, 784)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> Execute this as per the provided instructions, will remove 'j' and 'z' symbols\n",
        "</br>\n",
        ">> This results in having a total of 24 classes."
      ],
      "metadata": {
        "id": "yNYR6XCFPtnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alphabet=list(string.ascii_lowercase)\n",
        "alphabet.remove('j')\n",
        "alphabet.remove('z')\n",
        "print('Number of classes: ',len(alphabet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7Lqgs5PieI6",
        "outputId": "e16b3117-7ecc-4a9a-99cc-db5e450ca145"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of classes:  24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **5. Normalize data** </font>\n",
        ">> Normalization will be applied using mean and std of the training data set."
      ],
      "metadata": {
        "id": "xS2v7huWQNbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Current mean (training data set):', x_train.mean())\n",
        "print('Current standard deviation (training data set):', x_train.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEkM860Kknb5",
        "outputId": "a41ab7d2-b972-4aea-bfba-9f17ea7ee17d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current mean (training data set): 159.29083\n",
            "Current standard deviation (training data set): 48.76953\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> Create function to normalize data sets.\n",
        "</br>"
      ],
      "metadata": {
        "id": "GbGp8NDNRboY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#parameters include the mean, standard deviation and the data structure to normalize.\n",
        "def normalise(x_mean, x_std, x_data):\n",
        "    return (x_data - x_mean) / x_std"
      ],
      "metadata": {
        "id": "5O9TCWRIl_PO"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save mean and standard deviation our our training data set.\n",
        "x_mean = x_train.mean()\n",
        "x_std = x_train.std()"
      ],
      "metadata": {
        "id": "_rO-qxQYmhSQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> We will create a copy of each our samples data frames, that wat we can use such structures to plot our original images. These _raw_ dataframes are:\n",
        ">>> *x_train_raw* </br>\n",
        ">>> *x_val_raw*  </br>\n",
        ">>> *x_test_raw*  </br>"
      ],
      "metadata": {
        "id": "ww3_UthkSY3e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">> We will normalize our three samples dataframe and assign the normalize data to them. Dataframes are kept as:\n",
        ">>> *x_train* </br>\n",
        ">>> *x_val*  </br>\n",
        ">>> *x_test* </br>"
      ],
      "metadata": {
        "id": "EgA6q328TEI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_raw = x_train.copy()\n",
        "x_val_raw = x_val.copy()\n",
        "x_test_raw = x_test.copy()\n",
        "\n",
        "x_train = normalise(x_mean, x_std, x_train)\n",
        "x_val = normalise(x_mean, x_std, x_val)\n",
        "x_test = normalise(x_mean, x_std, x_test)"
      ],
      "metadata": {
        "id": "oyq6Q5R1SUHr"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('[x_train] mean after normalization:',x_train.mean())\n",
        "print('[x_train] std deviation after normalization:',x_train.std())\n",
        "print('[x_val] mean after normalization:',x_val.mean())\n",
        "print('[x_val] std deviation after normalization:',x_val.std())\n",
        "print('[x_test] mean after normalization:',x_test.mean())\n",
        "print('[x_test] std deviation after normalization:',x_test.std())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_CZPQ7HmnJn",
        "outputId": "72b31a5d-9e5c-4590-863b-33321470ab21"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[x_train] mean after normalization: 3.6268384e-06\n",
            "[x_train] std deviation after normalization: 0.99999946\n",
            "[x_val] mean after normalization: 0.025469628044303\n",
            "[x_val] std deviation after normalization: 1.0034091159237561\n",
            "[x_test] mean after normalization: 0.026902554344109323\n",
            "[x_test] std deviation after normalization: 1.0078364225306247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **6. Create mini batches function** </font>\n",
        ">>"
      ],
      "metadata": {
        "id": "RfvQtNZp7xpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Data wll be shuffled by default unless specified in the arguments when invoking the function.\n",
        "def createMiniBatches(batchSize, x, y, shuffle = True):\n",
        "    #throw error if provided samples and number of results(classification do not match)\n",
        "    assert x.shape[0] == y.shape[0], 'ERROR:: Number of Samples and classification results do not match!!!'\n",
        "    dataCount = x.shape[0]\n",
        "    #Shuffle data using indexes as helpers to maintian correct mappiing between a row and its calssification.\n",
        "    if shuffle:\n",
        "        indexes = np.arange(dataCount)\n",
        "        np.random.shuffle(indexes)\n",
        "        x = x[indexes]\n",
        "        y = y[indexes]\n",
        "    # usea for loop to return n mini batches, using the batchSize parameters to set selection pacing.\n",
        "    return ((x[i:i+batchSize], y[i:i+batchSize]) for i in range(0, dataCount, batchSize))"
      ],
      "metadata": {
        "id": "w-5CRzVJ7zvx"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **7. Create helper tensor class** </font>\n",
        ">>  AS numpy ndarray."
      ],
      "metadata": {
        "id": "ez4Kxx0_Wsvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class np_tensor(np.ndarray): pass"
      ],
      "metadata": {
        "id": "yQ6jSXMR8WPa"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center><font color=\"darkorange\"> ***EQUATIONS*** </font></center>\n",
        "\n",
        "### <center> <font color=\"darkblue\"><b> Forward</b> </font></center>\n",
        "$$z^1 = W^1 X + b^1$$\n",
        "\n",
        "### <center> <font color=\"darkblue\"><b> ReLU</b> </font></center>\n",
        "$$a^1 = ReLU(z^1) $$\n",
        "\n",
        "### <center><font color=\"darkblue\"><b> Backward</b> </font></center>\n",
        "$$z^2 = W^2 a^1 + b^2$$\n",
        "\n",
        "### <center><font color=\"darkblue\"><b> SoftMax Activation function</b> </font></center>\n",
        "\n",
        "$$\\hat{y} = \\frac{e^{z^{2_k}}}{\\sum_j{e^{z_j}}}$$\n",
        "\n",
        "### <center><font color=\"darkblue\"><b> Cost Function</b> </font></center>\n",
        "\n",
        "$$ \\mathcal{L}(\\hat{y}^{i}, y^{i}) =  - y^{i}  \\ln(\\hat{y}^{i}) = -\\ln(\\hat{y}^i)$$\n",
        "\n",
        "\n",
        "$$ \\mathcal{J}(w, b) =  \\frac{1}{num\\_samples} \\sum_{i=1}^{num\\_samples}-\\ln(\\hat{y}^{i})$$"
      ],
      "metadata": {
        "id": "5ZPmL-GLXmNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **8. Create _Linear()_ class**</font>"
      ],
      "metadata": {
        "id": "XGu4DUH_XCX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear():\n",
        "    def __init__(self, input_size, output_size):\n",
        "        self.W = (np.random.randn(output_size, input_size) / np.sqrt(input_size/2)).view(np_tensor)\n",
        "        self.b = (np.zeros((output_size, 1))).view(np_tensor) #view as np_tensor instance\n",
        "    # Forward fucntion of linear class\n",
        "    def __call__(self, X):\n",
        "        Z = self.W @ X + self.b\n",
        "        return Z\n",
        "    # Backward fucntion of linear class\n",
        "    def backward(self, X, Z):\n",
        "        X.grad = self.W.T @ Z.grad\n",
        "        self.W.grad = Z.grad @ X.T\n",
        "        self.b.grad = np.sum(Z.grad, axis = 1, keepdims=True)"
      ],
      "metadata": {
        "id": "PNFm7gd59boY"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **9. Create activation funciton as SoftMax and _Cost_ function**</font>"
      ],
      "metadata": {
        "id": "DKkUMrv0ZHZ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x, y):\n",
        "    #Number of records\n",
        "    xCount = x.shape[1]\n",
        "    #Calculate e to xn - applies to whole x\n",
        "    expX = np.exp(x)\n",
        "    #e(xn) over the sume of all e(x) values for the porbability\n",
        "    prob = expX / expX.sum(axis = 0)\n",
        "    # create a copy as predictions\n",
        "    preds = prob.copy()\n",
        "    # Cost f\n",
        "    y_pred = prob[y.squeeze(), np.arange(xCount)]\n",
        "    cost = np.sum(-np.log(y_pred)) / xCount\n",
        "    # Gradients\n",
        "    prob[y.squeeze(), np.arange(xCount)] -= 1 #dl/dx\n",
        "    x.grad = prob.copy()\n",
        "    #return predictions and cost\n",
        "    return preds, cost"
      ],
      "metadata": {
        "id": "SnTcLir39l6M"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **10. ReLU function**</font>"
      ],
      "metadata": {
        "id": "eO76IdAAk8b2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU():\n",
        "    def __call__(self, Z):\n",
        "        return np.maximum(0, Z)\n",
        "    def backward(self, Z, A):\n",
        "        Z.grad = A.grad.copy()\n",
        "        Z.grad[Z <= 0] = 0"
      ],
      "metadata": {
        "id": "e2u4AFfP9hyS"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **11. SequentialLayers class for model definition**</font>"
      ],
      "metadata": {
        "id": "-t042QtQlAjL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SequentialLayers():\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "        self.x = None\n",
        "        self.outputs = {}\n",
        "    def __call__(self, X):\n",
        "        self.x = X\n",
        "        self.outputs['l0'] = self.x\n",
        "        for i, layer in enumerate(self.layers, 1):\n",
        "            self.x = layer(self.x)\n",
        "            self.outputs['l'+str(i)]=self.x\n",
        "        return self.x\n",
        "    def backward(self):\n",
        "        for i in reversed(range(len(self.layers))):\n",
        "            self.layers[i].backward(self.outputs['l'+str(i)], self.outputs['l'+str(i+1)])\n",
        "    def update(self, learning_rate = 1e-3):\n",
        "        for layer in self.layers:\n",
        "            if isinstance(layer, ReLU): continue\n",
        "            layer.W = layer.W - learning_rate * layer.W.grad\n",
        "            layer.b = layer.b - learning_rate * layer.b.grad\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.__call__(X))"
      ],
      "metadata": {
        "id": "dYfPjGMK9inA"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **9. Acuuracy method**</font>\n",
        ">> Provide samples and their values, calculate accuracy based on the correct predicitons"
      ],
      "metadata": {
        "id": "m-K5s-8YkIOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(x, y, batchSize):\n",
        "    #Count the total numbe of predictions and the correct predictions\n",
        "    correctPredictions = 0\n",
        "    totalPredictions = 0\n",
        "    for i, (x, y) in enumerate(createMiniBatches(batchSize, x, y)):\n",
        "        pred = model(x.T.view(np_tensor))\n",
        "        # Increase correct count if correct!\n",
        "        correctPredictions += np.sum(np.argmax(pred, axis=0) == y.squeeze())\n",
        "        # Increase performed predictions on each iterarion\n",
        "        totalPredictions += pred.shape[1]\n",
        "    #Calculate accuaracy as correct predictions over total caluclations\n",
        "    return correctPredictions/totalPredictions"
      ],
      "metadata": {
        "id": "yxPiLtSn9r7m"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <font color=\"darkblue\"> **9. Created training method**</font>\n",
        ">> Parametrized training data sets as:\n",
        "\n",
        ">>> *   _trainingSetX_\n",
        ">>> *   _trainingSetY_\n",
        "\n",
        ">> Parametrized validation data sets as:\n",
        "\n",
        ">>> *   _trainingSetX_\n",
        ">>> *   _trainingSetY_\n",
        "\n",
        ">> Parametrized size of batch as _batchSize_ (default to 1e-3)\n",
        "</br>\n",
        ">> Parametrized learning rate as _learnRate_ (default to 512)"
      ],
      "metadata": {
        "id": "MPZS-i03iW-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defined default value of the batch size as 512 - after some testing it was the most useful size for this problem.\n",
        "# Defined default learningRate of the batch size as 512 - after some testing it was the most useful size for this problem.\n",
        "\n",
        "# Added two parameters two set the training datasets, we can use this parameters to point to other datasets if needed.\n",
        "def train(model, epochs, trainingSetX, trainingSetY, validationSetX, validationSetY, batchSize=512, learnRate = 1e-3, ):\n",
        "    for epoch in range(epochs):\n",
        "        print('Running epoch [',epoch,']')\n",
        "        # train using minibatches generated from the x and y training sets\n",
        "        for i, (x, y) in enumerate(createMiniBatches(batchSize, trainingSetX, trainingSetY)):\n",
        "            scores = model(x.T.view(np_tensor))\n",
        "            _, cost = softmax(scores, y)\n",
        "            # Run backward and update model\n",
        "            model.backward()\n",
        "            model.update(learnRate)\n",
        "        # calcualte accuracy of the model against the validation sets\n",
        "        print(f'Costo[{cost}]    Accuracy[{accuracy(validationSetX, validationSetY, batchSize)}]')"
      ],
      "metadata": {
        "id": "dEHDdQWA9pYu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <center><font color=\"darkorange\"> ***TRAINING*** </font></center>\n",
        "\n",
        ">> Set up our model using the sequentialLayers class\n",
        ">> set batch size, learning rate and number of epochs"
      ],
      "metadata": {
        "id": "2gbzFmi6ksJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use 24 classes for calssification\n",
        "model = SequentialLayers([Linear(784, 200), ReLU(), Linear(200, 200), ReLU(), Linear(200, 24)])\n",
        "batchSize = 512\n",
        "learningRate = 1e-4\n",
        "epochs = 30\n"
      ],
      "metadata": {
        "id": "rLDqLVfP9vaQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, epochs, x_train, y_train, x_val, y_val, batchSize, learningRate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPX6PjgA9yrz",
        "outputId": "1b4c42ad-27ca-437b-c018-e9065a21d64c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running epoch [ 0 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#<center><font color=\"darkorange\"> ***TEST OUR METHOD USING THE TEST SETS:: CALCUALTE ACCURACY*** </font></center>"
      ],
      "metadata": {
        "id": "I0rijdLGlaqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy(x_test, y_test, batchSize))"
      ],
      "metadata": {
        "id": "smFwt5Hc92O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <center><font color=\"darkorange\"> Use our model to make a prediction of one of our test samples and copare against the acutal result."
      ],
      "metadata": {
        "id": "1T169XdaloXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idx = np.random.randint(len(y_test))\n",
        "plot_ASL(x_test_raw[idx,])\n",
        "pred = model.predict(x_test[idx].reshape(-1, 1))\n",
        "print(f'Predicted values as: [{pred}], Actual value as [{y_test[idx][0]}]')"
      ],
      "metadata": {
        "id": "A2sVfM3l96gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SOME TESTS USING DIFFERENT NUMBER OF LAYERS"
      ],
      "metadata": {
        "id": "TDKxahpFqOou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SequentialLayers([Linear(784, 100), ReLU(), Linear(100, 100), ReLU(), Linear(100, 24)])\n",
        "batchSize = 512\n",
        "learningRate = 1e-4\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "7TkXkzQ8ozIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, epochs, x_train, y_train, x_val, y_val, batchSize, learningRate)"
      ],
      "metadata": {
        "id": "xrFAfFUloxdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy(x_test, y_test, batchSize))"
      ],
      "metadata": {
        "id": "-cnMn-rupabD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SequentialLayers([Linear(784, 300), ReLU(), Linear(300, 300), ReLU(), Linear(300, 24)])\n",
        "batchSize = 512\n",
        "learningRate = 1e-4\n",
        "epochs = 30"
      ],
      "metadata": {
        "id": "wasz_RIUpns8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, epochs, x_train, y_train, x_val, y_val, batchSize, learningRate)"
      ],
      "metadata": {
        "id": "gVwwy7GrptS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy(x_test, y_test, batchSize))"
      ],
      "metadata": {
        "id": "mpVpaXZIpvt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6Lqjs6JvqKo8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}